{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee9abbc",
   "metadata": {},
   "source": [
    "# Validación en Producción con Datos Actualizados\n",
    "\n",
    "Este notebook:\n",
    "1. Carga datos actualizados de los datasets originales (más recientes que los usados en entrenamiento)\n",
    "2. Procesa y prepara los nuevos datos siguiendo el mismo pipeline\n",
    "3. Genera predicciones usando el modelo ganador\n",
    "4. Compara las predicciones con los datos reales actualizados\n",
    "5. Evalúa el rendimiento del modelo en datos completamente nuevos\n",
    "\n",
    "**Objetivo**: Validar que el modelo mantiene su rendimiento con datos fuera del período de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import joblib\n",
    "import holidays\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "\n",
    "print(\"Notebook de Validación en Producción\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa217f16",
   "metadata": {},
   "source": [
    "## 1. Identificar Período de Datos Nuevos\n",
    "\n",
    "Primero identificamos qué período se usó en entrenamiento/validación y qué datos nuevos tenemos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de validación usados en el notebook 08\n",
    "df_validation_anterior = pd.read_parquet('artifacts/data/train_models/features_validation.parquet')\n",
    "\n",
    "print(\"Período usado en validación anterior:\")\n",
    "print(f\"  Inicio: {df_validation_anterior['datetime'].min()}\")\n",
    "print(f\"  Fin:    {df_validation_anterior['datetime'].max()}\")\n",
    "print(f\"  Registros: {len(df_validation_anterior):,}\")\n",
    "\n",
    "# Definir fecha de inicio para datos nuevos (después del último dato de validación)\n",
    "FECHA_INICIO_NUEVOS = df_validation_anterior['datetime'].max() + pd.Timedelta(hours=1)\n",
    "\n",
    "print(f\"\\nBuscando datos nuevos desde: {FECHA_INICIO_NUEVOS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995a248",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos Actualizados de Fuentes Originales\n",
    "\n",
    "Cargamos los datos de las mismas fuentes que se usaron en el notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de validación usados en el notebook 09\n",
    "validation_data_path = Path('artifacts/data/validation_models/features_validation.parquet')\n",
    "if validation_data_path.exists():\n",
    "    df_validation_original = pd.read_parquet(validation_data_path)\n",
    "    validation_end = df_validation_original['datetime'].max()\n",
    "    print(f\"✓ Datos de validación cargados (NB09)\")\n",
    "    print(f\"  Período de validación terminó el: {validation_end}\")\n",
    "else:\n",
    " print(\"ADVERTENCIA: No se encontraron datos de validación del NB09\")\n",
    "    validation_end = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc05867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos meteorológicos\n",
    "print(\"\\nCargando datos meteorológicos...\")\n",
    "meteo_files = list(Path('../../data/climatologia/data_parquet_clean/meteo').glob('*.parquet'))\n",
    "if meteo_files:\n",
    "    df_meteo_full = pd.concat([pd.read_parquet(f) for f in meteo_files], ignore_index=True)\n",
    "    df_meteo_full['datetime'] = pd.to_datetime(df_meteo_full['datetime'])\n",
    "    df_meteo_full = df_meteo_full.sort_values('datetime').drop_duplicates(subset=['datetime'])\n",
    "    print(f\"  ✓ Cargados {len(df_meteo_full):,} registros\")\n",
    "    print(f\"  Período: {df_meteo_full['datetime'].min()} a {df_meteo_full['datetime'].max()}\")\n",
    "    print(f\"  Columnas: {len(df_meteo_full.columns)}\")\n",
    "else:\n",
    "    print(\"  ✗ No se encontraron archivos meteorológicos\")\n",
    "    df_meteo_full = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de precios\n",
    "print(\"\\nCargando datos de precios...\")\n",
    "precio_files = list(Path('../../data/precio_luz/data_parquet_clean').glob('*.parquet'))\n",
    "if precio_files:\n",
    "    df_precios_full = pd.concat([pd.read_parquet(f) for f in precio_files], ignore_index=True)\n",
    "    df_precios_full['datetime'] = pd.to_datetime(df_precios_full['datetime'])\n",
    "    df_precios_full = df_precios_full.sort_values('datetime').drop_duplicates(subset=['datetime'])\n",
    "    print(f\"  ✓ Cargados {len(df_precios_full):,} registros\")\n",
    "    print(f\"  Período: {df_precios_full['datetime'].min()} a {df_precios_full['datetime'].max()}\")\n",
    "else:\n",
    "    print(\"  ✗ No se encontraron archivos de precios\")\n",
    "    df_precios_full = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233adc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo datos nuevos (posteriores a la validación anterior)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILTRANDO DATOS NUEVOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_demanda_full is not None:\n",
    "    df_demanda_nuevos = df_demanda_full[df_demanda_full['datetime'] >= FECHA_INICIO_NUEVOS].copy()\n",
    "    print(f\"\\nDemanda - Datos nuevos: {len(df_demanda_nuevos):,} registros\")\n",
    "    if len(df_demanda_nuevos) > 0:\n",
    "        print(f\"  Desde: {df_demanda_nuevos['datetime'].min()}\")\n",
    "        print(f\"  Hasta: {df_demanda_nuevos['datetime'].max()}\")\n",
    "    else:\n",
    "        print(\"  No hay datos nuevos de demanda\")\n",
    "\n",
    "if df_meteo_full is not None:\n",
    "    df_meteo_nuevos = df_meteo_full[df_meteo_full['datetime'] >= FECHA_INICIO_NUEVOS].copy()\n",
    "    print(f\"\\nMeteo - Datos nuevos: {len(df_meteo_nuevos):,} registros\")\n",
    "    if len(df_meteo_nuevos) > 0:\n",
    "        print(f\"  Desde: {df_meteo_nuevos['datetime'].min()}\")\n",
    "        print(f\"  Hasta: {df_meteo_nuevos['datetime'].max()}\")\n",
    "\n",
    "if df_precios_full is not None:\n",
    "    df_precios_nuevos = df_precios_full[df_precios_full['datetime'] >= FECHA_INICIO_NUEVOS].copy()\n",
    "    print(f\"\\nPrecios - Datos nuevos: {len(df_precios_nuevos):,} registros\")\n",
    "    if len(df_precios_nuevos) > 0:\n",
    "        print(f\"  Desde: {df_precios_nuevos['datetime'].min()}\")\n",
    "        print(f\"  Hasta: {df_precios_nuevos['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf397ad",
   "metadata": {},
   "source": [
    "## 3. Procesar Datos Nuevos\n",
    "\n",
    "Aplicamos el mismo procesamiento que en el notebook 01 y 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d79a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_caracteristicas_temporales(df):\n",
    "    \"\"\"\n",
    "    Crea características temporales basadas en la columna de datetime.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    # Características cíclicas\n",
    "    df['hora_del_dia_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour/24)\n",
    "    df['hora_del_dia_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour/24)\n",
    "    df['dia_semana_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek/7)\n",
    "    df['dia_semana_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek/7)\n",
    "    df['mes_sin'] = np.sin(2 * np.pi * df['datetime'].dt.month/12)\n",
    "    df['mes_cos'] = np.cos(2 * np.pi * df['datetime'].dt.month/12)\n",
    "\n",
    "    # Características categóricas\n",
    "    df['hora_del_dia'] = df['datetime'].dt.hour\n",
    "    df['dia_semana'] = df['datetime'].dt.dayofweek\n",
    "    df['mes'] = df['datetime'].dt.month\n",
    "    df['trimestre'] = df['datetime'].dt.quarter\n",
    "    df['año'] = df['datetime'].dt.year\n",
    "    df['es_festivo'] = df['datetime'].dt.date.isin(\n",
    "        holidays.Spain(years=df['datetime'].dt.year.unique().tolist())\n",
    "    ).astype(int)\n",
    "\n",
    "    # Características binarias\n",
    "    df['es_finde'] = df['dia_semana'].isin([5, 6]).astype(int)\n",
    "    df['es_laboral'] = (~df['dia_semana'].isin([5, 6])).astype(int)\n",
    "    df['es_hora_pico_mañana'] = df['hora_del_dia'].between(7, 9).astype(int)\n",
    "    df['es_hora_pico_tarde'] = df['hora_del_dia'].between(18, 20).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def crear_lags_y_medias_moviles(df, columna_target='real', lags=[1, 24, 48, 168]):\n",
    "    \"\"\"\n",
    "    Crea características de valores retardados y medias móviles.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Valores retardados\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}h'] = df[columna_target].shift(lag)\n",
    "    \n",
    "    # Medias móviles\n",
    "    ventanas = [6, 12, 24, 48, 168]\n",
    "    for ventana in ventanas:\n",
    "        df[f'media_movil_{ventana}h'] = df[columna_target].rolling(window=ventana).mean()\n",
    "        df[f'std_movil_{ventana}h'] = df[columna_target].rolling(window=ventana).std()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ea017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos datos históricos para calcular lags correctamente\n",
    "# Combinamos los últimos 7 días de los datos anteriores con los datos nuevos\n",
    "print(\"Preparando datos para calcular lags...\")\n",
    "\n",
    "fecha_inicio_contexto = FECHA_INICIO_NUEVOS - pd.Timedelta(days=7)\n",
    "df_demanda_contexto = df_demanda_full[\n",
    "    df_demanda_full['datetime'] >= fecha_inicio_contexto\n",
    "].copy()\n",
    "\n",
    "print(f\"  Datos con contexto: {len(df_demanda_contexto):,} registros\")\n",
    "print(f\"  Desde: {df_demanda_contexto['datetime'].min()}\")\n",
    "print(f\"  Hasta: {df_demanda_contexto['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear características temporales\n",
    "print(\"\\nCreando características temporales...\")\n",
    "df_demanda_procesado = crear_caracteristicas_temporales(df_demanda_contexto)\n",
    "print(f\"  ✓ Características temporales creadas\")\n",
    "\n",
    "# Crear lags y medias móviles\n",
    "print(\"\\nCreando lags y medias móviles...\")\n",
    "df_demanda_procesado = crear_lags_y_medias_moviles(df_demanda_procesado, columna_target='real')\n",
    "print(f\"  ✓ Lags y medias móviles creadas\")\n",
    "\n",
    "# Renombrar columna 'real' a 'demanda' para consistencia\n",
    "df_demanda_procesado = df_demanda_procesado.rename(columns={'real': 'demanda'})\n",
    "\n",
    "print(f\"\\nDatos procesados: {df_demanda_procesado.shape}\")\n",
    "print(f\"Columnas: {len(df_demanda_procesado.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02a594",
   "metadata": {},
   "source": [
    "## 4. Agregar Datos Meteorológicos\n",
    "\n",
    "Agregamos los datos meteorológicos usando los mismos pesos de población que en entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35039fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesos de población (los mismos que en el entrenamiento)\n",
    "pesos_ciudades = {\n",
    "    'albacete': 0.008943,\n",
    "    'alicante': 0.045363,\n",
    "    'almeria': 0.016711,\n",
    "    'avila': 0.003591,\n",
    "    'badajoz': 0.015198,\n",
    "    'barcelona': 0.133783,\n",
    "    'bilbao': 0.026242,\n",
    "    'burgos': 0.008017,\n",
    "    'caceres': 0.008784,\n",
    "    'cadiz': 0.028428,\n",
    "    'castellon': 0.013640,\n",
    "    'ciudad_real': 0.011110,\n",
    "    'cordoba': 0.017591,\n",
    "    'a_coruna': 0.025292,\n",
    "    'cuenca': 0.004426,\n",
    "    'girona': 0.017908,\n",
    "    'granada': 0.021231,\n",
    "    'guadalajara': 0.006007,\n",
    "    'huelva': 0.012194,\n",
    "    'huesca': 0.005036,\n",
    "    'jaen': 0.014028,\n",
    "    'leon': 0.010183,\n",
    "    'lleida': 0.010002,\n",
    "    'logrono': 0.007207,\n",
    "    'lugo': 0.007452,\n",
    "    'madrid': 0.159364,\n",
    "    'malaga': 0.040151,\n",
    "    'murcia': 0.035573,\n",
    "    'ourense': 0.006888,\n",
    "    'oviedo': 0.022764,\n",
    "    'palencia': 0.003545,\n",
    "    'pontevedra': 0.021340,\n",
    "    'salamanca': 0.007430,\n",
    "    'san_sebastian': 0.016259,\n",
    "    'santander': 0.013323,\n",
    "    'segovia': 0.003478,\n",
    "    'sevilla': 0.044466,\n",
    "    'soria': 0.001987,\n",
    "    'tarragona': 0.019308,\n",
    "    'teruel': 0.003026,\n",
    "    'toledo': 0.016101,\n",
    "    'valencia': 0.061656,\n",
    "    'valladolid': 0.011698,\n",
    "    'vitoria': 0.007520,\n",
    "    'zamora': 0.003794,\n",
    "    'zaragoza': 0.021950\n",
    "}\n",
    "\n",
    "def agregar_datos_meteo(weather_df, pesos_ciudades):\n",
    "    \"\"\"\n",
    "    Agrega los datos meteorológicos usando una media ponderada por población.\n",
    "    \"\"\"\n",
    "    # Obtener las columnas base (sin sufijos de ciudad)\n",
    "    cols_base = ['temperature_2m', 'precipitation', 'cloud_cover', 'wind_speed_10m']\n",
    "    \n",
    "    # Normalizar pesos para que sumen 1\n",
    "    total = sum(pesos_ciudades.values())\n",
    "    pesos_norm = {k: v/total for k, v in pesos_ciudades.items()}\n",
    "    \n",
    "    # Inicializar DataFrame agregado\n",
    "    weather_agg = pd.DataFrame()\n",
    "    weather_agg['datetime'] = weather_df['datetime'].unique()\n",
    "    \n",
    "    # Para cada variable meteorológica base\n",
    "    for col_base in cols_base:\n",
    "        weighted_sum = pd.Series(0, index=weather_agg.index)\n",
    "        total_weight = 0\n",
    "        \n",
    "        # Sumar las contribuciones ponderadas de cada ciudad\n",
    "        for ciudad, peso in pesos_norm.items():\n",
    "            col_ciudad = f\"{col_base}_{ciudad}\"\n",
    "            if col_ciudad in weather_df.columns:\n",
    "                weighted_sum += weather_df[col_ciudad].values * peso\n",
    "                total_weight += peso\n",
    "        \n",
    "        # Normalizar por el peso total efectivo\n",
    "        if total_weight > 0:\n",
    "            weather_agg[col_base] = weighted_sum / total_weight\n",
    "    \n",
    "    return weather_agg\n",
    "\n",
    "# Filtrar datos meteorológicos con contexto\n",
    "df_meteo_contexto = df_meteo_full[\n",
    "    df_meteo_full['datetime'] >= fecha_inicio_contexto\n",
    "].copy()\n",
    "\n",
    "# Agregar datos meteorológicos\n",
    "print(\"Agregando datos meteorológicos...\")\n",
    "df_meteo_agg = agregar_datos_meteo(df_meteo_contexto, pesos_ciudades)\n",
    "print(f\"  ✓ Datos meteorológicos agregados: {df_meteo_agg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e9c18",
   "metadata": {},
   "source": [
    "## 5. Combinar Todos los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a32402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar datos de precios con contexto\n",
    "df_precios_contexto = df_precios_full[\n",
    "    df_precios_full['datetime'] >= fecha_inicio_contexto\n",
    "].copy()\n",
    "\n",
    "print(\"Combinando todos los datos...\")\n",
    "\n",
    "# Merge con meteo\n",
    "df_completo = df_demanda_procesado.merge(df_meteo_agg, on='datetime', how='left')\n",
    "print(f\"  Después de merge con meteo: {df_completo.shape}\")\n",
    "\n",
    "# Merge con precios\n",
    "df_completo = df_completo.merge(df_precios_contexto, on='datetime', how='left')\n",
    "print(f\"  Después de merge con precios: {df_completo.shape}\")\n",
    "\n",
    "# Interpolar valores faltantes\n",
    "print(\"\\nInterpolando valores faltantes...\")\n",
    "n_nan_antes = df_completo.isna().sum().sum()\n",
    "df_completo = df_completo.interpolate(method='linear')\n",
    "n_nan_despues = df_completo.isna().sum().sum()\n",
    "print(f\"  NaN antes: {n_nan_antes}, después: {n_nan_despues}\")\n",
    "\n",
    "print(f\"\\nDataset completo: {df_completo.shape}\")\n",
    "print(f\"Columnas: {len(df_completo.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo los datos nuevos (sin el contexto histórico)\n",
    "df_nuevos = df_completo[df_completo['datetime'] >= FECHA_INICIO_NUEVOS].copy()\n",
    "\n",
    "# Eliminar filas con NaN\n",
    "df_nuevos = df_nuevos.dropna()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATOS NUEVOS LISTOS PARA VALIDACIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Registros: {len(df_nuevos):,}\")\n",
    "print(f\"Período: {df_nuevos['datetime'].min()} a {df_nuevos['datetime'].max()}\")\n",
    "print(f\"Columnas: {len(df_nuevos.columns)}\")\n",
    "print(\"\\nPrimeros registros:\")\n",
    "display(df_nuevos.head())\n",
    "print(\"\\nÚltimos registros:\")\n",
    "display(df_nuevos.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3de22",
   "metadata": {},
   "source": [
    "## 6. Cargar Modelo Ganador y Hacer Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar recomendación del modelo\n",
    "with open('artifacts/analysis/model_recommendation.json', 'r') as f:\n",
    "    recommendation = json.load(f)\n",
    "\n",
    "modelo_ganador = recommendation['best_model']\n",
    "print(f\"Modelo ganador: {modelo_ganador}\")\n",
    "print(f\"\\nMétricas en validación anterior:\")\n",
    "for metric, value in recommendation['metrics'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:,.2f}\" if metric != 'r2' else f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Cargar el pipeline del modelo\n",
    "model_path = Path('artifacts/trained_models') / recommendation['model_file']\n",
    "print(f\"\\nCargando modelo desde: {model_path}\")\n",
    "pipeline = joblib.load(model_path)\n",
    "print(\"✓ Modelo cargado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56eef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para predicción\n",
    "if hasattr(pipeline, 'feature_names_in_'):\n",
    "    required_features = pipeline.feature_names_in_\n",
    "    print(f\"Features requeridas por el modelo: {len(required_features)}\")\n",
    "    \n",
    "    # Verificar que tenemos todas las features\n",
    "    missing_features = set(required_features) - set(df_nuevos.columns)\n",
    "    if missing_features:\n",
    "        print(f\"\\nFeatures faltantes: {missing_features}\")\n",
    "        print(\"\\nNo se pueden hacer predicciones sin estas features.\")\n",
    "    else:\n",
    "        print(\"✓ Todas las features requeridas están disponibles\")\n",
    "        \n",
    "        # Preparar X e y\n",
    "        X_nuevos = df_nuevos[required_features].copy()\n",
    "        y_real = df_nuevos['demanda'].copy()\n",
    "        datetime_index = df_nuevos['datetime'].copy()\n",
    "        \n",
    "        print(f\"\\nDatos preparados:\")\n",
    "        print(f\"  X shape: {X_nuevos.shape}\")\n",
    "        print(f\"  y shape: {y_real.shape}\")\n",
    "else:\n",
    "    print(\"El modelo no tiene feature_names_in_\")\n",
    "    required_features = [col for col in df_nuevos.columns if col not in ['datetime', 'demanda']]\n",
    "    X_nuevos = df_nuevos[required_features].copy()\n",
    "    y_real = df_nuevos['demanda'].copy()\n",
    "    datetime_index = df_nuevos['datetime'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones\n",
    "print(\"\\nHaciendo predicciones...\")\n",
    "y_pred = pipeline.predict(X_nuevos)\n",
    "print(\"✓ Predicciones completadas\")\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_resultados = pd.DataFrame({\n",
    "    'datetime': datetime_index.values,\n",
    "    'demanda_real': y_real.values,\n",
    "    'demanda_predicha': y_pred,\n",
    "    'error': y_real.values - y_pred,\n",
    "    'error_abs': np.abs(y_real.values - y_pred),\n",
    "    'error_porcentual': np.abs((y_real.values - y_pred) / y_real.values) * 100\n",
    "})\n",
    "\n",
    "print(\"\\nPrimeras predicciones:\")\n",
    "display(df_resultados.head(10))\n",
    "print(\"\\nÚltimas predicciones:\")\n",
    "display(df_resultados.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857dfb9d",
   "metadata": {},
   "source": [
    "## 7. Evaluar Rendimiento en Datos Nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5af65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas\n",
    "mae = mean_absolute_error(y_real, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_real, y_pred))\n",
    "mape = mean_absolute_percentage_error(y_real, y_pred) * 100\n",
    "r2 = r2_score(y_real, y_pred)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"EVALUACIÓN EN DATOS NUEVOS (PRODUCCIÓN)\")\n",
    "print(f\"Período: {df_resultados['datetime'].min()} a {df_resultados['datetime'].max()}\")\n",
    "print(f\"Registros: {len(df_resultados):,}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MAE:  {mae:,.2f} MW\")\n",
    "print(f\"RMSE: {rmse:,.2f} MW\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comparar con métricas de validación anterior\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACIÓN CON VALIDACIÓN ANTERIOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mae_anterior = recommendation['metrics']['mae']\n",
    "rmse_anterior = recommendation['metrics']['rmse']\n",
    "mape_anterior = recommendation['metrics']['mape']\n",
    "r2_anterior = recommendation['metrics']['r2']\n",
    "\n",
    "print(f\"\\nMAE:\")\n",
    "print(f\"  Validación anterior: {mae_anterior:,.2f} MW\")\n",
    "print(f\"  Datos nuevos:        {mae:,.2f} MW\")\n",
    "print(f\"  Diferencia:          {mae - mae_anterior:+,.2f} MW ({((mae - mae_anterior)/mae_anterior)*100:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nRMSE:\")\n",
    "print(f\"  Validación anterior: {rmse_anterior:,.2f} MW\")\n",
    "print(f\"  Datos nuevos:        {rmse:,.2f} MW\")\n",
    "print(f\"  Diferencia:          {rmse - rmse_anterior:+,.2f} MW ({((rmse - rmse_anterior)/rmse_anterior)*100:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nMAPE:\")\n",
    "print(f\"  Validación anterior: {mape_anterior:.2f}%\")\n",
    "print(f\"  Datos nuevos:        {mape:.2f}%\")\n",
    "print(f\"  Diferencia:          {mape - mape_anterior:+.2f}%\")\n",
    "\n",
    "print(f\"\\nR²:\")\n",
    "print(f\"  Validación anterior: {r2_anterior:.4f}\")\n",
    "print(f\"  Datos nuevos:        {r2:.4f}\")\n",
    "print(f\"  Diferencia:          {r2 - r2_anterior:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Determinar si el modelo mantiene su rendimiento\n",
    "umbral_degradacion = 0.10  # 10% de degradación aceptable\n",
    "degradacion_mae = abs((mae - mae_anterior) / mae_anterior)\n",
    "degradacion_mape = abs((mape - mape_anterior) / mape_anterior)\n",
    "\n",
    "if degradacion_mae < umbral_degradacion and degradacion_mape < umbral_degradacion:\n",
    "    print(\"✓ El modelo MANTIENE su rendimiento en datos nuevos\")\n",
    "elif degradacion_mae < umbral_degradacion * 2 and degradacion_mape < umbral_degradacion * 2:\n",
    "    print(\"El modelo tiene una LIGERA degradación en datos nuevos\")\n",
    "else:\n",
    "    print(\"✗ El modelo tiene una DEGRADACIÓN SIGNIFICATIVA en datos nuevos\")\n",
    "    print(\"   Se recomienda reentrenar el modelo con datos más recientes\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5422f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas detalladas de errores\n",
    "print(\"\\nEstadísticas de errores:\")\n",
    "print(\"\\nError absoluto (MW):\")\n",
    "print(df_resultados['error_abs'].describe())\n",
    "print(\"\\nError porcentual (%):\")\n",
    "print(df_resultados['error_porcentual'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ea205",
   "metadata": {},
   "source": [
    "## 8. Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34108ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones vs real - Período completo\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "# Gráfico 1: Predicciones vs Real\n",
    "axes[0].plot(df_resultados['datetime'], df_resultados['demanda_real'], \n",
    "             label='Demanda Real', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(df_resultados['datetime'], df_resultados['demanda_predicha'], \n",
    "             label='Demanda Predicha', linewidth=2, alpha=0.8, linestyle='--')\n",
    "axes[0].set_title(f'Validación en Producción - {modelo_ganador}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Demanda (MW)', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 2: Error\n",
    "axes[1].plot(df_resultados['datetime'], df_resultados['error'], \n",
    "             color='red', linewidth=1, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].fill_between(df_resultados['datetime'], df_resultados['error'], \n",
    "                       alpha=0.3, color='red')\n",
    "axes[1].set_title('Error de Predicción', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Error (MW)', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 3: Error Porcentual\n",
    "axes[2].plot(df_resultados['datetime'], df_resultados['error_porcentual'], \n",
    "             color='orange', linewidth=1, alpha=0.7)\n",
    "axes[2].axhline(y=mape, color='red', linestyle='--', alpha=0.5, \n",
    "                label=f'MAPE medio: {mape:.2f}%')\n",
    "axes[2].set_title('Error Porcentual', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Fecha', fontsize=12)\n",
    "axes[2].set_ylabel('Error (%)', fontsize=12)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar primera semana en detalle (si hay suficientes datos)\n",
    "if len(df_resultados) >= 168:\n",
    "    primera_semana = df_resultados.head(168)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Predicciones vs Real\n",
    "    axes[0].plot(primera_semana['datetime'], primera_semana['demanda_real'], \n",
    "                 label='Demanda Real', linewidth=2, marker='o', markersize=3)\n",
    "    axes[0].plot(primera_semana['datetime'], primera_semana['demanda_predicha'], \n",
    "                 label='Demanda Predicha', linewidth=2, marker='s', markersize=3, linestyle='--')\n",
    "    axes[0].set_title('Primera Semana - Detalle', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Demanda (MW)', fontsize=12)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Error\n",
    "    axes[1].bar(primera_semana['datetime'], primera_semana['error'], \n",
    "                color=['red' if e < 0 else 'green' for e in primera_semana['error']], \n",
    "                alpha=0.6, width=0.03)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_title('Error - Primera Semana', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Fecha', fontsize=12)\n",
    "    axes[1].set_ylabel('Error (MW)', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No hay suficientes datos para mostrar una semana completa ({len(df_resultados)} horas disponibles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de errores y scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma de errores\n",
    "axes[0].hist(df_resultados['error'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].axvline(x=df_resultados['error'].mean(), color='green', linestyle='--', \n",
    "                linewidth=2, label=f'Media: {df_resultados[\"error\"].mean():.2f} MW')\n",
    "axes[0].set_title('Distribución de Errores', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Error (MW)', fontsize=12)\n",
    "axes[0].set_ylabel('Frecuencia', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Scatter plot: Real vs Predicho\n",
    "axes[1].scatter(df_resultados['demanda_real'], df_resultados['demanda_predicha'], \n",
    "                alpha=0.5, s=20, color='steelblue')\n",
    "# Línea diagonal perfecta\n",
    "min_val = min(df_resultados['demanda_real'].min(), df_resultados['demanda_predicha'].min())\n",
    "max_val = max(df_resultados['demanda_real'].max(), df_resultados['demanda_predicha'].max())\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicción Perfecta')\n",
    "axes[1].set_title('Demanda Real vs Predicha', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Demanda Real (MW)', fontsize=12)\n",
    "axes[1].set_ylabel('Demanda Predicha (MW)', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98461c67",
   "metadata": {},
   "source": [
    "## 9. Análisis por Hora del Día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar hora del día\n",
    "df_resultados['hora'] = pd.to_datetime(df_resultados['datetime']).dt.hour\n",
    "\n",
    "# Agrupar por hora\n",
    "errores_por_hora = df_resultados.groupby('hora').agg({\n",
    "    'error_abs': ['mean', 'std'],\n",
    "    'error_porcentual': 'mean',\n",
    "    'demanda_real': 'mean',\n",
    "    'demanda_predicha': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "errores_por_hora.columns = ['hora', 'mae', 'std', 'mape', 'demanda_real_media', 'demanda_pred_media']\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Error absoluto medio por hora con barras de error\n",
    "axes[0].bar(errores_por_hora['hora'], errores_por_hora['mae'], \n",
    "            yerr=errores_por_hora['std'], capsize=5,\n",
    "            color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Error Absoluto Medio por Hora del Día', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Hora del Día', fontsize=12)\n",
    "axes[0].set_ylabel('MAE (MW)', fontsize=12)\n",
    "axes[0].set_xticks(range(24))\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Demanda media real vs predicha por hora\n",
    "axes[1].plot(errores_por_hora['hora'], errores_por_hora['demanda_real_media'], \n",
    "             marker='o', linewidth=2, label='Real', markersize=8)\n",
    "axes[1].plot(errores_por_hora['hora'], errores_por_hora['demanda_pred_media'], \n",
    "             marker='s', linewidth=2, label='Predicha', markersize=8, linestyle='--')\n",
    "axes[1].set_title('Demanda Media por Hora del Día', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Hora del Día', fontsize=12)\n",
    "axes[1].set_ylabel('Demanda Media (MW)', fontsize=12)\n",
    "axes[1].set_xticks(range(24))\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError medio por hora del día:\")\n",
    "display(errores_por_hora[['hora', 'mae', 'mape']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c9e70",
   "metadata": {},
   "source": [
    "## 10. Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a30b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio para resultados de producción\n",
    "output_dir = Path('artifacts/production_validation')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Timestamp para los archivos\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Guardar predicciones\n",
    "predictions_file = output_dir / f'predictions_{timestamp}.parquet'\n",
    "df_resultados.to_parquet(predictions_file, index=False)\n",
    "print(f\"✓ Predicciones guardadas en: {predictions_file}\")\n",
    "\n",
    "# Guardar métricas\n",
    "metricas_produccion = {\n",
    "    'timestamp': timestamp,\n",
    "    'fecha_inicio': df_resultados['datetime'].min().isoformat(),\n",
    "    'fecha_fin': df_resultados['datetime'].max().isoformat(),\n",
    "    'n_registros': len(df_resultados),\n",
    "    'modelo': modelo_ganador,\n",
    "    'metricas_produccion': {\n",
    "        'mae': float(mae),\n",
    "        'rmse': float(rmse),\n",
    "        'mape': float(mape),\n",
    "        'r2': float(r2)\n",
    "    },\n",
    "    'metricas_validacion_anterior': recommendation['metrics'],\n",
    "    'comparacion': {\n",
    "        'mae_diff': float(mae - mae_anterior),\n",
    "        'mae_diff_pct': float(((mae - mae_anterior) / mae_anterior) * 100),\n",
    "        'rmse_diff': float(rmse - rmse_anterior),\n",
    "        'rmse_diff_pct': float(((rmse - rmse_anterior) / rmse_anterior) * 100),\n",
    "        'mape_diff': float(mape - mape_anterior),\n",
    "        'r2_diff': float(r2 - r2_anterior)\n",
    "    },\n",
    "    'rendimiento': 'BUENO' if degradacion_mae < umbral_degradacion else 'DEGRADADO'\n",
    "}\n",
    "\n",
    "metrics_file = output_dir / f'metrics_{timestamp}.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metricas_produccion, f, indent=2)\n",
    "print(f\"✓ Métricas guardadas en: {metrics_file}\")\n",
    "\n",
    "# Guardar análisis por hora\n",
    "errores_hora_file = output_dir / f'errores_por_hora_{timestamp}.csv'\n",
    "errores_por_hora.to_csv(errores_hora_file, index=False)\n",
    "print(f\"✓ Análisis por hora guardado en: {errores_hora_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDACIÓN EN PRODUCCIÓN COMPLETADA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Período validado: {df_resultados['datetime'].min()} a {df_resultados['datetime'].max()}\")\n",
    "print(f\"Registros: {len(df_resultados):,}\")\n",
    "print(f\"MAE: {mae:,.2f} MW (vs {mae_anterior:,.2f} MW en validación)\")\n",
    "print(f\"MAPE: {mape:.2f}% (vs {mape_anterior:.2f}% en validación)\")\n",
    "print(f\"\\nResultados guardados en: {output_dir}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
