{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2942808",
   "metadata": {},
   "source": [
    "# Pipeline Maestro - Orquestador de Notebooks\n",
    "\n",
    "Este notebook define todas las configuraciones centralizadas y ejecuta el pipeline completo de notebooks en orden.\n",
    "\n",
    "## Instrucciones de Uso\n",
    "\n",
    "### Opción 1: Ejecución Automatizada (Recomendada)\n",
    "\n",
    "1. **Configurar fechas** en la celda \"Configuración Central de Fechas\"\n",
    "2. **Configurar notebooks a ejecutar** en la celda \"Configuración de Ejecución\"\n",
    "3. **Ejecutar todas las celdas** hasta \"Ejecución del Pipeline Completo\"\n",
    "4. **Para NB07 (Neural Networks)**:\n",
    "   - Si `EJECUTAR_NB07_EN_COLAB = True`: El pipeline se detendrá automáticamente\n",
    "   - Sigue las instrucciones para ejecutar NB07 en Google Colab con GPU\n",
    "   - Descarga los resultados y cópialos a la carpeta `artifacts/`\n",
    "   - Ejecuta la celda \"Verificación de Resultados de NB07\"\n",
    "   - Ejecuta la celda \"Continuación del Pipeline\" para NB08-10\n",
    "5. **Revisar resultados** en la última celda\n",
    "\n",
    "### Opción 2: Ejecución Manual\n",
    "\n",
    "1. **Abrir cada notebook individualmente** (01-10)\n",
    "2. **Modificar las fechas** en las celdas de parámetros de cada notebook\n",
    "3. **Ejecutar todas las celdas** de cada notebook en orden\n",
    "4. **NB07 opcional**: Ejecutar en Google Colab para aprovechar GPU\n",
    "\n",
    "### Opción 3: Ejecución Híbrida\n",
    "\n",
    "1. **Ejecutar algunos notebooks manualmente** para exploración\n",
    "2. **Usar este NB00** para automatizar el resto del pipeline\n",
    "\n",
    "## Configuración Centralizada\n",
    "\n",
    "Define aquí una sola vez:\n",
    "- **Rango de fechas para generación de datos** (NB01, NB02)\n",
    "- **Fecha de corte Train/Validation** (NB03)\n",
    "- **Rango de fechas para entrenamiento** (NB04, NB05, NB06, NB07)\n",
    "\n",
    "## Pipeline de Ejecución\n",
    "\n",
    "1. **NB01** - Data Preparation: Carga y preprocesa datos de fuentes\n",
    "2. **NB02** - Feature Engineering: Crea características temporales y lags\n",
    "3. **NB03** - Exploratory Analysis: Separa train/validation, selecciona features\n",
    "4. **NB04** - Baseline Models: Entrena modelos básicos de referencia\n",
    "5. **NB05** - Machine Learning: Entrena modelos tree-based con hiperparámetros manuales\n",
    "6. **NB06** - Hyperparameter Optimization: Optimiza con Optuna (opcional)\n",
    "7. **NB07** - Neural Networks: Entrena redes neuronales (MLP, LSTM, CNN-LSTM) - **RECOMENDADO EN GOOGLE COLAB CON GPU**\n",
    "8. **NB08** - Model Comparison: Compara todos los modelos\n",
    "9. **NB09** - Model Validation: Valida modelo ganador en datos futuros\n",
    "10. **NB10** - Production Validation: Valida con datos actualizados de producción\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Configuración única y centralizada\n",
    "- Ejecución automatizada de todo el pipeline\n",
    "- Consistencia en fechas entre todos los notebooks\n",
    "- Fácil modificación de parámetros\n",
    "- Reproducibilidad completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8e7469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDICCIÓN DE DEMANDA ELÉCTRICA\n",
      "================================================================================\n",
      "\n",
      "Fecha de ejecución: 2025-10-26 19:45:02\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREDICCIÓN DE DEMANDA ELÉCTRICA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFecha de ejecución: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac609c",
   "metadata": {},
   "source": [
    "## Gestión de Backup de Artifacts\n",
    "\n",
    "**OPCIONAL**: Guarda el contenido actual de `artifacts/` antes de ejecutar el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af861d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haciendo backup de artifacts...\n",
      "   Origen: artifacts\n",
      "   Destino: ../../.backup_pipeline/2025-10-26_19-45-07/artifacts\n",
      "[OK] Backup completado: ../../.backup_pipeline/2025-10-26_19-45-07\n",
      "     Archivos copiados: 32\n",
      "\n",
      "Limpiando artifacts...\n",
      "[OK] Artifacts limpiado. Listo para nueva ejecución.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACIÓN DE BACKUP\n",
    "# ============================================================================\n",
    "\n",
    "# Cambiar a True si quieres hacer backup del artifacts actual antes de ejecutar\n",
    "# True para guardar backup, False para no hacer nada\n",
    "HACER_BACKUP_ARTIFACTS = True\n",
    "\n",
    "# ============================================================================\n",
    "# GESTIÓN AUTOMÁTICA DE BACKUP\n",
    "# ============================================================================\n",
    "\n",
    "if HACER_BACKUP_ARTIFACTS:\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Crear nombre de carpeta con fecha y hora actual\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    backup_dir = Path('../../.backup_pipeline') / timestamp\n",
    "    artifacts_dir = Path('artifacts')\n",
    "    \n",
    "    # Verificar si existe la carpeta artifacts\n",
    "    if artifacts_dir.exists() and any(artifacts_dir.iterdir()):\n",
    "        print(f\"Haciendo backup de artifacts...\")\n",
    "        print(f\"   Origen: {artifacts_dir}\")\n",
    "        print(f\"   Destino: {backup_dir / 'artifacts'}\")\n",
    "        \n",
    "        # Crear directorio de backup y copiar artifacts\n",
    "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copytree(artifacts_dir, backup_dir / 'artifacts', dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"[OK] Backup completado: {backup_dir}\")\n",
    "        print(f\"     Archivos copiados: {sum(1 for _ in (backup_dir / 'artifacts').rglob('*') if _.is_file())}\")\n",
    "        \n",
    "        # Borrar contenido de artifacts (pero mantener la carpeta)\n",
    "        print(f\"\\nLimpiando artifacts...\")\n",
    "        for item in artifacts_dir.iterdir():\n",
    "            if item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "            else:\n",
    "                item.unlink()\n",
    "        \n",
    "        print(f\"[OK] Artifacts limpiado. Listo para nueva ejecución.\\n\")\n",
    "    else:\n",
    "        print(f\"[INFO] No hay contenido en artifacts/ para hacer backup.\\n\")\n",
    "else:\n",
    "    print(f\"[INFO] Backup desactivado (HACER_BACKUP_ARTIFACTS = False)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050316e",
   "metadata": {},
   "source": [
    "## Configuración Central de Fechas\n",
    "\n",
    "**IMPORTANTE**: Modifica estas fechas según tus necesidades. Todos los notebooks usarán estos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0055a824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURACIÓN DE FECHAS\n",
      "================================================================================\n",
      "\n",
      "1. Generación de Datos (NB01, NB02):\n",
      "   Inicio: Desde el principio\n",
      "   Fin:    2025-09-20 00:00:00\n",
      "\n",
      "2. Separación Train/Validation (NB03):\n",
      "   Corte:  2025-09-21 00:00:00\n",
      "   Train:  datos < 2025-09-21 00:00:00\n",
      "   Val:    datos >= 2025-09-21 00:00:00\n",
      "\n",
      "3. Entrenamiento de Modelos (NB04-NB07):\n",
      "   Inicio: 2023-01-01 00:00:00\n",
      "   Fin:    2025-09-20 00:00:00\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACIÓN CENTRAL - MODIFICA ESTAS VARIABLES SEGÚN NECESITES\n",
    "# ============================================================================\n",
    "\n",
    "# 1. RANGO DE DATOS PARA PROCESAMIENTO (NB01, NB02)\n",
    "# Controla qué datos se cargan y procesan desde las fuentes originales\n",
    "# None = desde el principio\n",
    "FECHA_INICIO_DATOS = None\n",
    "# Última fecha de datos a procesar\n",
    "FECHA_FIN_DATOS = pd.Timestamp('2025-09-20')\n",
    "# 2. FECHA DE CORTE TRAIN/VALIDATION (NB03)\n",
    "# Define dónde termina entrenamiento y empieza validación\n",
    "FECHA_CORTE_TRAIN_VAL = pd.Timestamp('2025-09-21 00:00:00')\n",
    "\n",
    "# 3. RANGO PARA ENTRENAMIENTO DE MODELOS (NB04, NB05, NB06, NB07)\n",
    "# Permite usar solo un subconjunto del dataset de entrenamiento\n",
    "# None = usar todo el dataset de train\n",
    "FECHA_INICIO_ENTRENAMIENTO = pd.Timestamp('2023-01-01')\n",
    "# Última fecha de entrenamiento\n",
    "FECHA_FIN_ENTRENAMIENTO = pd.Timestamp('2025-09-20')\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"CONFIGURACIÓN DE FECHAS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Generación de Datos (NB01, NB02):\")\n",
    "print(f\"   Inicio: {'Desde el principio' if FECHA_INICIO_DATOS is None else FECHA_INICIO_DATOS}\")\n",
    "print(f\"   Fin:    {FECHA_FIN_DATOS}\")\n",
    "\n",
    "print(\"\\n2. Separación Train/Validation (NB03):\")\n",
    "print(f\"   Corte:  {FECHA_CORTE_TRAIN_VAL}\")\n",
    "print(f\"   Train:  datos < {FECHA_CORTE_TRAIN_VAL}\")\n",
    "print(f\"   Val:    datos >= {FECHA_CORTE_TRAIN_VAL}\")\n",
    "\n",
    "print(\"\\n3. Entrenamiento de Modelos (NB04-NB07):\")\n",
    "print(f\"   Inicio: {'Usar todo el train' if FECHA_INICIO_ENTRENAMIENTO is None else FECHA_INICIO_ENTRENAMIENTO}\")\n",
    "print(f\"   Fin:    {FECHA_FIN_ENTRENAMIENTO}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff10804",
   "metadata": {},
   "source": [
    "## Configuración de Ejecución\n",
    "\n",
    "Define qué notebooks ejecutar y configuraciones adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fdb918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTEBOOKS A EJECUTAR:\n",
      "================================================================================\n",
      "  [SÍ]  01_data_preparation.ipynb\n",
      "  [SÍ]  02_feature_engineering.ipynb\n",
      "  [SÍ]  03_exploratory_analysis.ipynb\n",
      "  [SÍ]  04_baseline_models.ipynb\n",
      "  [SÍ]  05_models_machine_learning.ipynb\n",
      "  [SÍ]  06_hyperparameter_optimization.ipynb\n",
      "  [SÍ]  07_models_neural_networks.ipynb\n",
      "  [SÍ]  08_model_comparison.ipynb\n",
      "  [SÍ]  09_model_validation.ipynb\n",
      "  [NO]  10_production_validation.ipynb\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACIÓN DE EJECUCIÓN\n",
    "# ============================================================================\n",
    "\n",
    "# Notebooks a ejecutar (marca False los que quieras saltar)\n",
    "EJECUTAR_NOTEBOOKS = {\n",
    "    '01_data_preparation': True,\n",
    "    '02_feature_engineering': True,\n",
    "    '03_exploratory_analysis': True,\n",
    "    '04_baseline_models': True,\n",
    "    '05_models_machine_learning': True,\n",
    "    '06_hyperparameter_optimization': True,\n",
    "    # Ejecutamos en Colab porque es pesado, pero podría ser True si tu máquina local puede\n",
    "    '07_models_neural_networks': True,\n",
    "    '08_model_comparison': True,\n",
    "    '09_model_validation': True\n",
    "}\n",
    "\n",
    "# IMPORTANTE: Configuración especial para NB07 (Neural Networks)\n",
    "# Este notebook es pesado y se recomienda ejecutarlo en Google Colab con GPU\n",
    "# True = detener antes de NB07 para ejecución manual en Colab\n",
    "EJECUTAR_NB07_EN_COLAB = True\n",
    "\n",
    "# Directorio de notebooks\n",
    "NOTEBOOKS_DIR = Path('.')\n",
    "\n",
    "# Directorio para guardar notebooks ejecutados (con outputs)\n",
    "OUTPUT_DIR = Path('artifacts/executed_notebooks')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Mostrar configuración\n",
    "print(\"\\nNOTEBOOKS A EJECUTAR:\")\n",
    "print(\"=\" * 80)\n",
    "for nb_name, ejecutar in EJECUTAR_NOTEBOOKS.items():\n",
    "    status = \"[SÍ]\" if ejecutar else \"[NO]\"\n",
    "    print(f\"  {status}  {nb_name}.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12b3ea",
   "metadata": {},
   "source": [
    "## Verificación de Dependencias\n",
    "\n",
    "Verifica que papermill esté instalado para ejecutar notebooks programáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8f03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Papermill instalado correctamente\n",
      "  Versión: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "# Verificar que papermill está instalado\n",
    "try:\n",
    "    import papermill as pm\n",
    "    print(\"[OK] Papermill instalado correctamente\")\n",
    "    print(f\"  Versión: {pm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"[ERROR] Papermill no está instalado\")\n",
    "    print(\"\\nPara instalar papermill, ejecuta:\")\n",
    "    print(\"  pip install papermill\")\n",
    "    print(\"\\nO descomenta la siguiente línea:\")\n",
    "    # !pip install papermill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330cdb0",
   "metadata": {},
   "source": [
    "## Función de Ejecución de Notebooks\n",
    "\n",
    "Función helper que ejecuta un notebook con los parámetros configurados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1176249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_notebook(notebook_name, parameters=None):\n",
    "    \"\"\"\n",
    "    Ejecuta un notebook con papermill.\n",
    "    \n",
    "    Args:\n",
    "        notebook_name: Nombre del notebook sin extensión\n",
    "        parameters: Diccionario con parámetros a inyectar\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (bool éxito, str mensaje_error)\n",
    "    \"\"\"\n",
    "    input_path = NOTEBOOKS_DIR / f\"{notebook_name}.ipynb\"\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        error_msg = f\"No se encontró {input_path}\"\n",
    "        print(f\"    [ERROR] {error_msg}\")\n",
    "        return False, error_msg\n",
    "    \n",
    "    try:\n",
    "        print(f\"    Ejecutando...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Construir paths directamente como literales para evitar caché de papermill\n",
    "        pm.execute_notebook(\n",
    "            input_path=f\"./{notebook_name}.ipynb\",\n",
    "            output_path=f\"artifacts/executed_notebooks/{notebook_name}_executed.ipynb\",\n",
    "            parameters=parameters or {},\n",
    "            kernel_name='python3',\n",
    "            progress_bar=False\n",
    "        )\n",
    "        \n",
    "        output_str = f\"artifacts/executed_notebooks/{notebook_name}_executed.ipynb\"\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"    [OK] Completado en {duration:.1f}s\")\n",
    "        print(f\"    Output guardado: {output_str}\")\n",
    "        return True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"    [ERROR] Falló la ejecución\")\n",
    "        print(f\"    Tipo de error: {type(e).__name__}\")\n",
    "        print(f\"    Mensaje: {error_msg}\")\n",
    "        \n",
    "        # Recalcular output_path para logging\n",
    "        output_path = OUTPUT_DIR / f\"{notebook_name}_executed.ipynb\"\n",
    "        print(f\"    Notebook con error guardado en: {output_path}\")\n",
    "        \n",
    "        # Guardar información del error en un archivo de log\n",
    "        error_log_path = OUTPUT_DIR / f\"{notebook_name}_ERROR.txt\"\n",
    "        with open(error_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"ERROR EN: {notebook_name}.ipynb\\n\")\n",
    "            f.write(f\"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Tipo: {type(e).__name__}\\n\")\n",
    "            f.write(f\"Mensaje: {error_msg}\\n\")\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"DETALLES COMPLETOS DEL ERROR:\\n\")\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "            import traceback\n",
    "            f.write(traceback.format_exc())\n",
    "        \n",
    "        print(f\"    Detalles del error guardados en: {error_log_path}\")\n",
    "        \n",
    "        return False, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c392f43",
   "metadata": {},
   "source": [
    "## Ejecución del Pipeline Completo\n",
    "\n",
    "Ejecuta todos los notebooks en orden con los parámetros configurados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INICIANDO EJECUCIÓN DEL PIPELINE\n",
      "================================================================================\n",
      "\n",
      "[18:49:57] Ejecutando: 01_data_preparation.ipynb\n",
      "    Ejecutando...\n",
      "    [OK] Completado en 2.7s\n",
      "    Output guardado: artifacts/executed_notebooks/01_data_preparation_executed.ipynb\n",
      "\n",
      "[18:50:00] Ejecutando: 02_feature_engineering.ipynb\n",
      "    Ejecutando...\n",
      "    [OK] Completado en 4.5s\n",
      "    Output guardado: artifacts/executed_notebooks/02_feature_engineering_executed.ipynb\n",
      "\n",
      "[18:50:04] Ejecutando: 03_exploratory_analysis.ipynb\n",
      "    Ejecutando...\n",
      "    [OK] Completado en 110.8s\n",
      "    Output guardado: artifacts/executed_notebooks/03_exploratory_analysis_executed.ipynb\n",
      "\n",
      "[18:51:55] Ejecutando: 04_baseline_models.ipynb\n",
      "    Ejecutando...\n",
      "    [OK] Completado en 3.4s\n",
      "    Output guardado: artifacts/executed_notebooks/04_baseline_models_executed.ipynb\n",
      "\n",
      "[18:51:58] Ejecutando: 05_models_machine_learning.ipynb\n",
      "    Ejecutando...\n",
      "    [OK] Completado en 18.2s\n",
      "    Output guardado: artifacts/executed_notebooks/05_models_machine_learning_executed.ipynb\n",
      "\n",
      "[18:52:17] Ejecutando: 06_hyperparameter_optimization.ipynb\n",
      "    Ejecutando...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Ejecutando: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m exito, error_msg \u001b[38;5;241m=\u001b[39m \u001b[43mejecutar_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparametros\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m resultados[nb_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124méxito\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exito \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exito:\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36mejecutar_notebook\u001b[0;34m(notebook_name, parameters)\u001b[0m\n\u001b[1;32m     21\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Construir paths directamente como literales para evitar caché de papermill\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_notebook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnotebook_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ipynb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martifacts/executed_notebooks/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnotebook_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_executed.ipynb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/executed_notebooks/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnotebook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_executed.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m duration \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/papermill/execute.py:116\u001b[0m, in \u001b[0;36mexecute_notebook\u001b[0;34m(input_path, output_path, parameters, engine_name, request_save_on_cell_execute, prepare_only, kernel_name, language, progress_bar, log_output, stdout_file, stderr_file, start_timeout, report_mode, cwd, **engine_kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Execute the Notebook in `cwd` if it is set\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m chdir(cwd):\n\u001b[0;32m--> 116\u001b[0m     nb \u001b[38;5;241m=\u001b[39m \u001b[43mpapermill_engines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_notebook_with_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_save_on_cell_execute\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdout_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstderr_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Check for errors first (it saves on error before raising)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m raise_for_execution_errors(nb, output_path)\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/papermill/engines.py:48\u001b[0m, in \u001b[0;36mPapermillEngines.execute_notebook_with_engine\u001b[0;34m(self, engine_name, nb, kernel_name, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_notebook_with_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m, engine_name, nb, kernel_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetch a named engine and execute the nb object against it.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/papermill/engines.py:370\u001b[0m, in \u001b[0;36mEngine.execute_notebook\u001b[0;34m(cls, nb, kernel_name, output_path, progress_bar, log_output, autosave_cell_every, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m nb_man\u001b[38;5;241m.\u001b[39mnotebook_start()\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_managed_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_man\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     nb_man\u001b[38;5;241m.\u001b[39mcleanup_pbar()\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/papermill/engines.py:442\u001b[0m, in \u001b[0;36mNBClientEngine.execute_managed_notebook\u001b[0;34m(cls, nb_man, kernel_name, log_output, stdout_file, stderr_file, start_timeout, execution_timeout, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Nicely handle preprocessor arguments prioritizing values set by engine\u001b[39;00m\n\u001b[1;32m    432\u001b[0m final_kwargs \u001b[38;5;241m=\u001b[39m merge_kwargs(\n\u001b[1;32m    433\u001b[0m     safe_kwargs,\n\u001b[1;32m    434\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mexecution_timeout \u001b[38;5;28;01mif\u001b[39;00m execution_timeout \u001b[38;5;28;01melse\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     stderr_file\u001b[38;5;241m=\u001b[39mstderr_file,\n\u001b[1;32m    441\u001b[0m )\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPapermillNotebookClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_man\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinal_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/papermill/clientwrap.py:45\u001b[0m, in \u001b[0;36mPapermillNotebookClient.execute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_kernel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting notebook with kernel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpapermill_execute_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     info_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_reply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkc\u001b[38;5;241m.\u001b[39mkernel_info())\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m info_msg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/papermill/clientwrap.py:72\u001b[0m, in \u001b[0;36mPapermillNotebookClient.papermill_execute_cells\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_man\u001b[38;5;241m.\u001b[39mcell_start(cell, index)\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CellExecutionError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_man\u001b[38;5;241m.\u001b[39mcell_exception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells[index], cell_index\u001b[38;5;241m=\u001b[39mindex, exception\u001b[38;5;241m=\u001b[39mex)\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:171\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _runner_map:\n\u001b[1;32m    170\u001b[0m     _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runner_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MIA/workspace/TFM/.venv/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:128\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__runner_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    127\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.19/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.19/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Función helper para convertir Timestamps a strings (papermill necesita tipos JSON serializables)\n",
    "def convertir_parametros(params):\n",
    "    \"\"\"Convierte pd.Timestamp a string para que sean serializables por papermill\"\"\"\n",
    "    converted = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            converted[key] = value.isoformat()\n",
    "        elif value is None:\n",
    "            converted[key] = None\n",
    "        else:\n",
    "            converted[key] = value\n",
    "    return converted\n",
    "\n",
    "# Preparar parámetros para cada grupo de notebooks\n",
    "parametros_nb01_nb02 = convertir_parametros({\n",
    "    'FECHA_INICIO_DATOS': FECHA_INICIO_DATOS,\n",
    "    'FECHA_FIN_DATOS': FECHA_FIN_DATOS\n",
    "})\n",
    "\n",
    "parametros_nb03 = convertir_parametros({\n",
    "    'FECHA_CORTE_TRAIN_VAL': FECHA_CORTE_TRAIN_VAL\n",
    "})\n",
    "\n",
    "parametros_nb04_nb07 = convertir_parametros({\n",
    "    'FECHA_INICIO_ENTRENAMIENTO': FECHA_INICIO_ENTRENAMIENTO,\n",
    "    'FECHA_FIN_ENTRENAMIENTO': FECHA_FIN_ENTRENAMIENTO\n",
    "})\n",
    "\n",
    "# Mapeo de notebooks a parámetros\n",
    "notebooks_config = [\n",
    "    ('01_data_preparation', parametros_nb01_nb02),\n",
    "    ('02_feature_engineering', parametros_nb01_nb02),\n",
    "    ('03_exploratory_analysis', parametros_nb03),\n",
    "    ('04_baseline_models', parametros_nb04_nb07),\n",
    "    ('05_models_machine_learning', parametros_nb04_nb07),\n",
    "    ('06_hyperparameter_optimization', parametros_nb04_nb07),\n",
    "    ('07_models_neural_networks', parametros_nb04_nb07),\n",
    "    ('08_model_comparison', {}),\n",
    "    ('09_model_validation', {})\n",
    "]\n",
    "\n",
    "# Ejecutar pipeline\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INICIANDO EJECUCIÓN DEL PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resultados = {}\n",
    "inicio_pipeline = datetime.now()\n",
    "\n",
    "for nb_name, parametros in notebooks_config:\n",
    "    # Verificar si llegamos al NB07 y está configurado para ejecutarse en Colab\n",
    "    if nb_name == '07_models_neural_networks' and EJECUTAR_NB07_EN_COLAB:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"[PAUSA] PAUSA EN EL PIPELINE - NB07 REQUIERE GOOGLE COLAB\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\nINSTRUCCIONES PARA EJECUTAR NB07 EN GOOGLE COLAB:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n[1] PREPARAR ARCHIVOS PARA COLAB:\")\n",
    "        print(\"   Sube los siguientes archivos a tu Google Drive manteniendo la estructura completa:\")\n",
    "        print(\"   TFM/models/demand_forecast/\")\n",
    "        print(\"   \")\n",
    "        print(\"   - artifacts/data/train_models/\")\n",
    "        print(\"      └── features_train.parquet\")\n",
    "        print(\"   \")\n",
    "        print(\"   - artifacts/analysis/\")\n",
    "        print(\"      └── selected_features.json\")\n",
    "        print(\"   \")\n",
    "        print(\"   - artifacts/trained_models/\")\n",
    "        print(\"      └── tree_models_results.csv\")\n",
    "        \n",
    "        print(\"\\n[2] ABRIR NB07 EN GOOGLE COLAB:\")\n",
    "        print(\"   • Ve a https://colab.research.google.com\")\n",
    "        print(\"   • File > Upload notebook\")\n",
    "        print(\"   • Selecciona: 07_models_neural_networks.ipynb\")\n",
    "        \n",
    "        print(\"\\n[3] CONFIGURAR GPU EN COLAB:\")\n",
    "        print(\"   • Runtime > Change runtime type\")\n",
    "        print(\"   • Hardware accelerator: GPU (T4 recomendado)\")\n",
    "        print(\"   • Save\")\n",
    "        \n",
    "        print(\"\\n[4] EJECUTAR NB07 EN COLAB:\")\n",
    "        print(\"   • La celda 3 montará Google Drive (autoriza el acceso)\")\n",
    "        print(\"   • Ejecuta todas las celdas secuencialmente\")\n",
    "        print(\"   • Al final, los resultados se copiarán automáticamente a Google Drive\")\n",
    "        \n",
    "        print(\"\\n[5] RECUPERAR RESULTADOS:\")\n",
    "        print(\"   Descarga de Google Drive manteniendo la estructura completa:\")\n",
    "        print(\"   TFM/models/demand_forecast/\")\n",
    "        print(\"   \")\n",
    "        print(\"   - artifacts/trained_models/\")\n",
    "        print(\"      ├── neural_models_results.csv\")\n",
    "        print(\"      ├── mlp_mejorado_pipeline.pkl\")\n",
    "        print(\"      ├── lstm_mejorado_pipeline.pkl\")\n",
    "        print(\"      └── cnn_lstm_mejorado_pipeline.pkl\")\n",
    "        print(\"   \")\n",
    "        print(\"   - artifacts/data/predictions/ (opcional)\")\n",
    "        print(\"      └── neural_*.parquet\")\n",
    "        \n",
    "        print(\"\\n[6] CONTINUAR PIPELINE:\")\n",
    "        print(\"   Los archivos descargados ya estarán en las ubicaciones correctas.\")\n",
    "        print(\"   Luego ejecuta la siguiente celda para continuar con NB08-10\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"[PAUSADO] PIPELINE PAUSADO - Esperando ejecución de NB07 en Colab\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        resultados[nb_name] = 'pendiente_colab'\n",
    "        \n",
    "        # Detener ejecución aquí\n",
    "        print(\"\\n[ACCION REQUERIDA] Para continuar:\")\n",
    "        print(\"   1. Ejecuta NB07 en Google Colab siguiendo las instrucciones\")\n",
    "        print(\"   2. Descarga y copia los resultados a artifacts/\")\n",
    "        print(\"   3. Ejecuta la celda de 'CONTINUACIÓN DEL PIPELINE' más abajo\")\n",
    "        \n",
    "        # Salir del loop aquí\n",
    "        break\n",
    "    \n",
    "    if not EJECUTAR_NOTEBOOKS.get(nb_name, False):\n",
    "        print(f\"\\n[SALTADO] {nb_name}.ipynb\")\n",
    "        resultados[nb_name] = 'saltado'\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Ejecutando: {nb_name}.ipynb\")\n",
    "    exito, error_msg = ejecutar_notebook(nb_name, parametros)\n",
    "    resultados[nb_name] = 'éxito' if exito else 'error'\n",
    "    \n",
    "    if not exito:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[ERROR CRÍTICO] EL PIPELINE SE HA DETENIDO\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nNotebook fallido: {nb_name}.ipynb\")\n",
    "        print(f\"Razón: {error_msg}\")\n",
    "        print(f\"\\nPara revisar el error:\")\n",
    "        print(f\"  1. Abre: artifacts/executed_notebooks/{nb_name}_executed.ipynb\")\n",
    "        print(f\"  2. Revisa: artifacts/executed_notebooks/{nb_name}_ERROR.txt\")\n",
    "        print(f\"\\nCorrige el error y vuelve a ejecutar el pipeline.\")\n",
    "        print(\"=\" * 80)\n",
    "        # Detener el pipeline\n",
    "        break \n",
    "\n",
    "    # Continuar con el siguiente notebook\n",
    "    continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETADO (HASTA NB06)\" if '07_models_neural_networks' in resultados and resultados['07_models_neural_networks'] == 'pendiente_colab' else \"PIPELINE COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "duracion_total = (datetime.now() - inicio_pipeline).total_seconds()\n",
    "print(f\"\\nTiempo total: {duracion_total / 60:.1f} minutos\")\n",
    "\n",
    "# Resumen de resultados\n",
    "print(\"\\nRESUMEN DE EJECUCIÓN:\")\n",
    "print(\"-\" * 80)\n",
    "exitos = sum(1 for r in resultados.values() if r == 'éxito')\n",
    "errores = sum(1 for r in resultados.values() if r == 'error')\n",
    "saltados = sum(1 for r in resultados.values() if r == 'saltado')\n",
    "pendientes = sum(1 for r in resultados.values() if r == 'pendiente_colab')\n",
    "\n",
    "for nb_name, resultado in resultados.items():\n",
    "    if resultado == 'éxito':\n",
    "        icono = \"[OK]\"\n",
    "    elif resultado == 'error':\n",
    "        icono = \"[ERROR]\"\n",
    "    elif resultado == 'pendiente_colab':\n",
    "        icono = \"[PAUSADO]\"\n",
    "    else:\n",
    "        icono = \"[SALTADO]\"\n",
    "    print(f\"  {icono} {nb_name}: {resultado}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total: {exitos} éxitos, {errores} errores, {saltados} saltados, {pendientes} pendientes (Colab)\")\n",
    "\n",
    "if pendientes > 0:\n",
    "    print(\"\\n[ACCION REQUERIDA]\")\n",
    "    print(\"   Ejecuta NB07 en Google Colab y luego ejecuta la celda de CONTINUACIÓN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa61cdf",
   "metadata": {},
   "source": [
    "## Verificación de Resultados de NB07 (Google Colab)\n",
    "\n",
    "**SOLO ejecuta esta celda después de:**\n",
    "1. Haber ejecutado NB07 en Google Colab\n",
    "2. Haber descargado y copiado los archivos de resultados a `artifacts/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que los archivos necesarios de NB07 existen\n",
    "print(\"=\" * 80)\n",
    "print(\"VERIFICACIÓN DE RESULTADOS DE NB07 (Google Colab)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "archivos_requeridos = [\n",
    "    'artifacts/trained_models/neural_models_results.csv',\n",
    "    'artifacts/trained_models/mlp_mejorado_pipeline.pkl',\n",
    "    'artifacts/trained_models/lstm_mejorado_pipeline.pkl',\n",
    "    'artifacts/trained_models/cnn_lstm_mejorado_pipeline.pkl'\n",
    "]\n",
    "\n",
    "todos_presentes = True\n",
    "print(\"\\nVerificando archivos requeridos:\")\n",
    "for archivo in archivos_requeridos:\n",
    "    archivo_path = Path(archivo)\n",
    "    existe = archivo_path.exists()\n",
    "    icono = \"[OK]\" if existe else \"[FALTA]\"\n",
    "    print(f\"  {icono} {archivo}\")\n",
    "    if not existe:\n",
    "        todos_presentes = False\n",
    "\n",
    "if todos_presentes:\n",
    "    print(\"\\n[OK] Todos los archivos de NB07 están presentes\")\n",
    "    print(\"    Puedes continuar con la siguiente celda\")\n",
    "    \n",
    "    # Actualizar estado de resultados\n",
    "    if '07_models_neural_networks' in resultados:\n",
    "        resultados['07_models_neural_networks'] = 'éxito_colab'\n",
    "else:\n",
    "    print(\"\\n[ERROR] FALTAN ARCHIVOS\")\n",
    "    print(\"   Por favor, completa la ejecución de NB07 en Google Colab\")\n",
    "    print(\"   y copia los archivos de resultados a artifacts/\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83e1c0",
   "metadata": {},
   "source": [
    "## Continuación del Pipeline (NB08-10)\n",
    "\n",
    "Ejecuta esta celda solo después de verificar que los archivos de NB07 están presentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuar con notebooks posteriores a NB07\n",
    "if '07_models_neural_networks' not in resultados or resultados['07_models_neural_networks'] != 'éxito_colab':\n",
    "    print(\"[ADVERTENCIA] Primero verifica los resultados de NB07 en la celda anterior\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CONTINUANDO PIPELINE - NOTEBOOKS 08-10\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Notebooks post-NB07\n",
    "    notebooks_post_colab = [\n",
    "        ('08_model_comparison', {}),\n",
    "        ('09_model_validation', {}),\n",
    "        ('10_production_validation', {}),\n",
    "    ]\n",
    "    \n",
    "    for nb_name, parametros in notebooks_post_colab:\n",
    "        if not EJECUTAR_NOTEBOOKS.get(nb_name, False):\n",
    "            print(f\"\\n[SALTADO] {nb_name}.ipynb\")\n",
    "            resultados[nb_name] = 'saltado'\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Ejecutando: {nb_name}.ipynb\")\n",
    "        exito, error_msg = ejecutar_notebook(nb_name, parametros)\n",
    "        resultados[nb_name] = 'éxito' if exito else 'error'\n",
    "        \n",
    "        if not exito:\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[ERROR CRÍTICO] EL PIPELINE SE HA DETENIDO\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"\\nNotebook fallido: {nb_name}.ipynb\")\n",
    "            print(f\"Razón: {error_msg}\")\n",
    "            print(f\"\\nPara revisar el error:\")\n",
    "            print(f\"  1. Abre: artifacts/executed_notebooks/{nb_name}_executed.ipynb\")\n",
    "            print(f\"  2. Revisa: artifacts/executed_notebooks/{nb_name}_ERROR.txt\")\n",
    "            print(f\"\\nCorrige el error y vuelve a ejecutar el pipeline.\")\n",
    "            print(\"=\" * 80)\n",
    "            break  # Detener el pipeline\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PIPELINE COMPLETO FINALIZADO\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Resumen final actualizado\n",
    "    duracion_total = (datetime.now() - inicio_pipeline).total_seconds()\n",
    "    print(f\"\\nTiempo total: {duracion_total / 60:.1f} minutos\")\n",
    "    \n",
    "    print(\"\\nRESUMEN FINAL DE EJECUCIÓN:\")\n",
    "    print(\"-\" * 80)\n",
    "    exitos = sum(1 for r in resultados.values() if r in ['éxito', 'éxito_colab'])\n",
    "    errores = sum(1 for r in resultados.values() if r == 'error')\n",
    "    saltados = sum(1 for r in resultados.values() if r == 'saltado')\n",
    "    \n",
    "    for nb_name, resultado in resultados.items():\n",
    "        if resultado in ['éxito', 'éxito_colab']:\n",
    "            icono = \"[OK]\"\n",
    "        elif resultado == 'error':\n",
    "            icono = \"[ERROR]\"\n",
    "        else:\n",
    "            icono = \"[SALTADO]\"\n",
    "        \n",
    "        # Mostrar (Colab) si fue ejecutado en Colab\n",
    "        label = f\"{nb_name} (Colab)\" if resultado == 'éxito_colab' else nb_name\n",
    "        print(f\"  {icono} {label}: {resultado if resultado != 'éxito_colab' else 'éxito'}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total: {exitos} éxitos, {errores} errores, {saltados} saltados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4c87a",
   "metadata": {},
   "source": [
    "## Resumen de Artefactos Generados\n",
    "\n",
    "Verifica qué archivos se generaron durante el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nARTEFACTOS GENERADOS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "artifacts_base = Path('artifacts')\n",
    "\n",
    "# Datos procesados (NB01)\n",
    "print(\"\\n1. Datos Procesados (NB01):\")\n",
    "processed_dir = artifacts_base / 'data' / 'processed'\n",
    "if processed_dir.exists():\n",
    "    for file in sorted(processed_dir.glob('*.parquet')):\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   [OK] {file.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"   [NO ENCONTRADO]\")\n",
    "\n",
    "# Features (NB02)\n",
    "print(\"\\n2. Features (NB02):\")\n",
    "features_dir = artifacts_base / 'data' / 'features'\n",
    "if features_dir.exists():\n",
    "    for file in sorted(features_dir.glob('*.parquet')):\n",
    "        size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   [OK] {file.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"   [NO ENCONTRADO]\")\n",
    "\n",
    "# Datasets Train/Val (NB03)\n",
    "print(\"\\n3. Datasets Train/Validation (NB03):\")\n",
    "train_dir = artifacts_base / 'data' / 'train_models'\n",
    "val_dir = artifacts_base / 'data' / 'validation_models'\n",
    "for dir_path, label in [(train_dir, 'Train'), (val_dir, 'Validation')]:\n",
    "    if dir_path.exists():\n",
    "        for file in sorted(dir_path.glob('*.parquet')):\n",
    "            size_mb = file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   [OK] {label}: {file.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Modelos entrenados (NB04-NB07)\n",
    "print(\"\\n4. Modelos Entrenados (NB04-NB07):\")\n",
    "models_dir = artifacts_base / 'trained_models'\n",
    "if models_dir.exists():\n",
    "    # Archivos CSV de resultados\n",
    "    for file in sorted(models_dir.glob('*_results.csv')):\n",
    "        print(f\"   [OK] {file.name}\")\n",
    "    # Pipelines de modelos\n",
    "    pipelines = list(models_dir.glob('*_pipeline.pkl'))\n",
    "    if pipelines:\n",
    "        print(f\"   [OK] {len(pipelines)} pipelines guardados\")\n",
    "else:\n",
    "    print(\"   [NO ENCONTRADO]\")\n",
    "\n",
    "# Análisis y recomendaciones (NB03, NB08)\n",
    "print(\"\\n5. Análisis y Recomendaciones:\")\n",
    "analysis_dir = artifacts_base / 'analysis'\n",
    "if analysis_dir.exists():\n",
    "    for file in sorted(analysis_dir.glob('*.json')):\n",
    "        print(f\"   [OK] {file.name}\")\n",
    "else:\n",
    "    print(\"   [NO ENCONTRADO]\")\n",
    "\n",
    "# Optuna (NB06, opcional)\n",
    "print(\"\\n6. Optimización Optuna (NB06, opcional):\")\n",
    "optuna_dir = artifacts_base / 'optuna'\n",
    "if optuna_dir.exists():\n",
    "    for file in sorted(optuna_dir.glob('*.json')):\n",
    "        print(f\"   [OK] {file.name}\")\n",
    "else:\n",
    "    print(\"   [OPCIONAL - NO EJECUTADO]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71a15e",
   "metadata": {},
   "source": [
    "## Modelo Recomendado\n",
    "\n",
    "Muestra el modelo ganador seleccionado por el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar recomendación del modelo\n",
    "recommendation_path = Path('artifacts/analysis/model_recommendation.json')\n",
    "\n",
    "if recommendation_path.exists():\n",
    "    with open(recommendation_path, 'r') as f:\n",
    "        recommendation = json.load(f)\n",
    "    \n",
    "    print(\"\\nMODELO RECOMENDADO PARA PRODUCCIÓN:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Soportar ambos formatos (español e inglés)\n",
    "    modelo = recommendation.get('modelo_recomendado') or recommendation.get('best_model')\n",
    "    metricas = recommendation.get('metricas') or recommendation.get('metrics')\n",
    "    \n",
    "    print(f\"\\n[GANADOR] Modelo ganador: {modelo}\")\n",
    "    print(\"\\nMétricas en Test Set:\")\n",
    "    \n",
    "    if metricas:\n",
    "        for metric, value in metricas.items():\n",
    "            if metric == 'tiempo_entrenamiento' or metric == 'training_time':\n",
    "                print(f\"  • Tiempo entrenamiento: {value:.2f}s\")\n",
    "            elif metric in ['mae', 'rmse']:\n",
    "                print(f\"  • {metric.upper()}: {value:.2f} MW\")\n",
    "            elif metric == 'mape':\n",
    "                print(f\"  • {metric.upper()}: {value:.2f}%\")\n",
    "            elif metric == 'r2':\n",
    "                print(f\"  • R²: {value:.4f}\")\n",
    "    \n",
    "    # Mostrar comparación de enfoques si está disponible\n",
    "    if 'comparacion_enfoques' in recommendation:\n",
    "        comp = recommendation['comparacion_enfoques']\n",
    "        print(\"\\nComparación de Enfoques:\")\n",
    "        print(f\"  • Manual (NB05):        MAE = {comp['manual_mae']:.2f} MW\")\n",
    "        print(f\"  • Optuna (NB06):        MAE = {comp.get('optuna_mae', 'N/A')}\")\n",
    "        print(f\"  • Neural Networks (NB07): MAE = {comp['neural_mae']:.2f} MW\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"\\n[ADVERTENCIA] No se encontró recomendación de modelo.\")\n",
    "    print(\"   Ejecuta el NB08 (model_comparison) para seleccionar el mejor modelo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
